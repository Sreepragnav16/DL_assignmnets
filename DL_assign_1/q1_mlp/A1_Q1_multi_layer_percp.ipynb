{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#np.random.seed(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mul_lay_prcp_layer():\n",
    "    def __init__(self, no_of_prev_lay_node, no_of_curr_lay_node, last_lay = False, bias=True):\n",
    "        self.bias_wt = np.zeros((no_of_curr_lay_node, 1)) if bias else None\n",
    "        self.weight = np.random.rand(no_of_prev_lay_node, no_of_curr_lay_node)       \n",
    "        self.no_of_prev_lay_node = no_of_prev_lay_node\n",
    "        self.no_of_curr_lay_node = no_of_curr_lay_node\n",
    "        self.last_lay = last_lay\n",
    "        \n",
    "        self.inp = None\n",
    "        self.out = None\n",
    "        self.delta_wt = None\n",
    "        self.delta_in = None\n",
    "        self.delta_b = None\n",
    "              \n",
    "    def lay_feed_forward(self, input):\n",
    "        self.inp = input\n",
    "        if self.bias_wt is not None:\n",
    "            op_before_act = np.dot(self.weight.T, input)\n",
    "        op_before_act = np.dot(self.weight.T, input)+self.bias_wt\n",
    "        self.op_before_act = op_before_act\n",
    "        self.out = op_before_act\n",
    "        op_after_act = 1/(1+np.exp(-op_before_act))\n",
    "        self.op_after_act = op_after_act\n",
    "        return op_before_act\n",
    "    \n",
    "    def wt_grad(self, next_lay_wt = None, next_lay_grad = None, actual_output = None):\n",
    "        if self.last_lay == False :\n",
    "            #print(\"entered false\")\n",
    "            self.delta_wt = (self.op_after_act*(1-self.op_after_act))*np.dot(next_lay_wt, next_lay_grad)\n",
    "        elif self.last_lay == True:\n",
    "            #print(\"entered true\")\n",
    "            self.delta_wt = (self.op_after_act*(1-self.op_after_act))*(self.op_after_act-actual_output.T)\n",
    "        self.delta_in = np.dot(self.inp, self.delta_wt.T)\n",
    "        if self.bias_wt is not None:\n",
    "            self.delta_b = np.sum(self.delta_wt, axis=-1, keepdims=True)/self.delta_wt.shape[-1]     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mul_lay_prcp():\n",
    "    def __init__(self, no_of_layers, layer_nodes):\n",
    "        self.no_of_layers = no_of_layers\n",
    "        self.layer_nodes = layer_nodes\n",
    "        self.mlp_layers = []\n",
    "        self.mlp_lay_out = []\n",
    "        self.loss = []\n",
    "        self.mlp_in = []\n",
    "        self.mlp_out = []\n",
    "        for lay_id in range(1,self.no_of_layers):\n",
    "            if ((lay_id < self.no_of_layers-1) and (lay_id > 0)):\n",
    "                self.mlp_layers.append(mul_lay_prcp_layer(self.layer_nodes[lay_id-1],self.layer_nodes[lay_id]))\n",
    "            elif (lay_id == self.no_of_layers-1):\n",
    "                self.mlp_layers.append(mul_lay_prcp_layer(self.layer_nodes[lay_id-1],self.layer_nodes[lay_id],last_lay = True))   \n",
    "    \n",
    "    def mlp_grad(self):\n",
    "        for lay_id in reversed(range(len(self.mlp_layers))):\n",
    "            #print(lay_id)\n",
    "            if lay_id == len(self.mlp_layers)-1 and self.mlp_layers[lay_id].last_lay == True:\n",
    "                self.mlp_layers[lay_id].wt_grad(actual_output = self.Y)\n",
    "            else:\n",
    "                self.mlp_layers[lay_id].wt_grad(next_lay_wt = self.mlp_layers[lay_id+1].weight, next_lay_grad = self.mlp_layers[lay_id+1].delta_wt)\n",
    "                \n",
    "    def train_mlp(self, X, Y, learn_rate, epochs):\n",
    "        X = X.T\n",
    "        Y = Y\n",
    "        self.learn_rate = learn_rate\n",
    "        count = int(epochs*0.1)\n",
    "        for epoch_num in range(epochs):\n",
    "            for x in range(X.shape[-1]):\n",
    "                self.X = X[:,x].reshape(-1,1)\n",
    "                self.Y = Y[x,:].reshape(1,-1)\n",
    "                #out = self.mlp_feed_forward(self.X)\n",
    "                mlp_f_out_1 = self.X\n",
    "                for mlp_layer_1 in self.mlp_layers:\n",
    "                    mlp_f_out_1 = 1/(1+np.exp(-(mlp_layer_1.lay_feed_forward(mlp_f_out_1))))\n",
    "                    self.mlp_out.append(mlp_f_out_1)\n",
    "                rm_sq_loss = self.root_mean_sqr_loss(mlp_f_out_1,self.Y)\n",
    "                self.loss.append(rm_sq_loss)\n",
    "                self.mlp_grad()\n",
    "                self.SGD()\n",
    "            #Z = self.mlp_feed_forward(X)\n",
    "            #print(Z.T.shape. Y.shape)\n",
    "            mlp_f_out_2 = X\n",
    "            for mlp_layer_2 in self.mlp_layers:\n",
    "                    mlp_f_out_2 = 1/(1+np.exp(-(mlp_layer_2.lay_feed_forward(mlp_f_out_2))))\n",
    "            #Z = mlp_f_out_2\n",
    "            accuracy = np.sum((mlp_f_out_2.T > 0.5) == Y)*100./Y.shape[0]\n",
    "            if epoch_num%count==0:\n",
    "                print(\"At epoch\",epoch_num)\n",
    "                print(\"Loss is \",self.loss[-1])\n",
    "                print(\"Accuracy is\",accuracy)\n",
    "    \n",
    "    def SGD(self):\n",
    "        for mlp_layer in self.mlp_layers:\n",
    "            #if mlp_layer.delta_in is not None:\n",
    "            mlp_layer.weight = mlp_layer.weight-(self.learn_rate*mlp_layer.delta_in)\n",
    "            if mlp_layer.delta_b is not None:\n",
    "                mlp_layer.bias_wt = mlp_layer.bias_wt-(self.learn_rate*mlp_layer.delta_b)\n",
    "            \n",
    "    def test_mlp(self,X):\n",
    "        X = X.T\n",
    "        #out = self.mlp_feed_forward(X)\n",
    "        out = X\n",
    "        for mlp_layer_out in self.mlp_layers:\n",
    "                    out = 1/(1+np.exp(-(mlp_layer_out.lay_feed_forward(out))))\n",
    "        predictions = (out > 0.5).astype(np.int)\n",
    "        return predictions\n",
    "                \n",
    "                \n",
    "    @staticmethod\n",
    "    def root_mean_sqr_loss(out,actual_output):\n",
    "        out = out.T\n",
    "        rm_sq_loss = (1/2)*np.sum(np.square(out - actual_output))\n",
    "        return rm_sq_loss\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mul_lyr_pcp = mul_lay_prcp(no_of_layers = 3, layer_nodes = [2,4,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [0,0],\n",
    "    [1,0],\n",
    "    [0,1],\n",
    "    [1,1]\n",
    "])\n",
    "Y = np.array([\n",
    "    [0],\n",
    "    [1],\n",
    "    [1],\n",
    "    [0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training phase of multi layer perceptron\n",
      "\n",
      "At epoch 0\n",
      "Loss is  0.3039439048158523\n",
      "Accuracy is 50.0\n",
      "At epoch 150\n",
      "Loss is  0.14884788883854203\n",
      "Accuracy is 25.0\n",
      "At epoch 300\n",
      "Loss is  0.14311461294076042\n",
      "Accuracy is 50.0\n",
      "At epoch 450\n",
      "Loss is  0.1402417279996087\n",
      "Accuracy is 50.0\n",
      "At epoch 600\n",
      "Loss is  0.13515398489163846\n",
      "Accuracy is 50.0\n",
      "At epoch 750\n",
      "Loss is  0.09359188417524367\n",
      "Accuracy is 75.0\n",
      "At epoch 900\n",
      "Loss is  0.08315453331391819\n",
      "Accuracy is 75.0\n",
      "At epoch 1050\n",
      "Loss is  0.008035527088264067\n",
      "Accuracy is 100.0\n",
      "At epoch 1200\n",
      "Loss is  0.002408388560238789\n",
      "Accuracy is 100.0\n",
      "At epoch 1350\n",
      "Loss is  0.0013389476011020618\n",
      "Accuracy is 100.0\n",
      "\n",
      "Testing phase of multi layer perceptron\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nTraining phase of multi layer perceptron\\n\")\n",
    "mul_lyr_pcp.train_mlp(X, Y, 1, 1500)\n",
    "print(\"\\nTesting phase of multi layer perceptron\")\n",
    "mul_lyr_pcp.test_mlp(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
